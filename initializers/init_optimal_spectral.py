#                           initOptimalSpectral.m
#
#  Initializer recently proposed based on an optimal spectral method. The
#  plain vanilla spectral initializer computes the largest eigenvector of Y
#  = 1/m sum(yi * ai * ai') for i = 1 to m.  The truncated version of this
#  method throws away some of the rows.  The optimal spectral nitializer
#  computes the largest eigenvector of Y = 1/m sum(T(yi) * ai * ai') for i
#  = 1 to m, where T(yi) is a function of yi given in equation (5) of the
#  paper cited below.
#
# I/O
#  Inputs:
#     A:  m x n matrix (or optionally a function handle to a method) that
#         returns A*x.
#     At: The adjoint (transpose) of 'A'. If 'A' is a function handle, 'At'
#         must be provided.
#     b0: m x 1 real,non-negative vector consists of all the measurements.
#     n:  The size of the unknown signal. It must be provided if A is a
#         function handle.
#     isTruncated (boolean): If true, use the 'truncated' initializer that
#                            uses a sub-sample of the measurement.
#     isScaled (boolean):    If true, use a least-squares method to
#                            determine  the optimal scale of the
#                            initializer.
#
#     Note: When a function handle is used, the value of 'n' (the length of
#     the unknown signal) and 'At' (a function handle for the adjoint of
#     'A') must be supplied.  When 'A' is numeric, the values of 'At' and
#     'n' are ignored and inferred from the arguments.
#
#  Outputs:
#     x0:  A n x 1 vector. It is the guess generated by the spectral method
#          for  the unknown signal.
#
#  See the script 'testInitOptimalSpectral.m' for an example of proper usage of
#  this function.
#
# Notation
#  Our notation follows the TWF paper.
#  ai is the conjugate transpose of the ith row of A.
#  yi is the ith element of y, which is the element-wise square of the
#  measurements b0.
#
# Algorithm Description.
#  Calculate the leading eigenvector of a matrix Y, where Y = 1/m sum(T(yi)
#  * ai * ai') for i = 1 to m, where T() is a "pre-processing" function.
#  The method return this leading eigenvector,
#  which is calculated using Matlab's eigs() routine.
#
#  Note: This implementation differs from the paper in several ways that
#  make it more efficient and robust.
#  The papers below recommend using the power method to compute the leading
#  eigenvector.  Our implemention
#  uses Matlab's built-in function eigs() to get the leading eigenvector
#  because of greater efficiency.
#
#  Also, the authors define the pre-processing function
#                 T(z) = (z-1)/(z+sqrt(delta)-1),
# where delta is the ratio of number of measurements to number of
# dimensions.  This formula assumes that the measurements are Gaussian with
# variance 1/n, and the unknown signal has length sqrt(n).  This assumption
# is clearly violated by most real sensing matrices and signals.  However,
# note that this measurements model yields measurements y = abs(Ax)^2 that
# have expected value E(y)=1.  For this reason, we normalize the
# measurements to have mean 1 before we apply the pre-processing function.
# We then multiply the mean back into the results when we're done
# pre-processing.

# References
#  Title:   Fundamental Limits of Weak Recovery with Applications to Phase
#           Retrieval
#  Place:   Equations (4) and (5)
#  Authors: Marco Mondelli, Andrea Montanari
#  Arxiv Address: https://arxiv.org/pdf/1708.05932.pdf
#

#
# PhasePack by Rohan Chandra, Ziyuan Zhong, Justin Hontz, Val McCulloch,
# Christoph Studer, & Tom Goldstein
# Copyright (c) University of Maryland, 2017

# -----------------------------START----------------------------------


import numpy as np
import math
import struct



def initOptimalSpectral(A=None, At=None, b0=None, n=None, isScaled=None, verbose=None, *args, **kwargs):
    # If A is a matrix, infer n and At from A. Then, transform matrix into
    # a function handle.
    # if A.isnumeric():
    #     n = np.size(A, 2)
    #     At = lambda x=None: np.dot(A.T, x)
    #     A = lambda x=None: np.dot(A, x)

    m = np.size(b0)
    print(n)

    if not(verbose) or verbose:
        print(['Estimating signal of length {0} using an orthogonal '.format(
            n)+'initializer with {0} measurements...\n'.format(m)])

    # Measurements as defined in the paper
    y = b0 ** 2
    delta = m / n

    # Normalize the measurements
    ymean = np.mean(y)
    y = y / ymean
    # print("y",y)
    # Apply pre-processing function
    yplus = max(y.any(), 0)
    T = (yplus - 1) / (yplus + math.sqrt(delta) - 1)

    # Un-normalize the measurements
    T = np.dot(T, ymean)
    # Build the function handle associated to the matrix Y
    Yfunc = lambda x=None: np.dot(1 / m, At(np.multiply(T, A(x))))
    # def Yfunc(x=None):
    #     print("x",x.shape)
    #     A_x = np.dot(A, x)
    #     re=np.dot(1 / m, At(np.multiply(T, A_x)))
    #     print(re)
    #     return re
    
    # Our implemention uses Matlab's built-in function eigs() to get the leading
# eigenvector because of greater efficiency.
# Create opts struct for eigs
    opts = struct
    opts.isreal = False
    # Get the eigenvector that corresponds to the largest eigenvalue of the associated matrix of Yfunc.
    # [x0,~] = eigs(Yfunc, n, 1, 'lr', opts);
    # x0, __ = eigs(Yfunc, n, 1, 'lr', opts, nargout=2)
    # print(Yfunc(y))
    '''
    x0 = np.linalg.eigh(Yfunc)
    '''
    x0 = np.random.random(size=256) + 1j*np.random.random(size=256)
    # This part does not appear in the Null paper. We add it for better
# performance. Rescale the solution to have approximately the correct
# magnitude
    if isScaled:
        b = b0
        Ax = abs(np.dot(A, x0))
        u = np.multiply(Ax, b)
        l = np.multiply(Ax, Ax)
        s = math.sqrt(np.dot(np.ravel(u), np.ravel(u))) / \
            math.sqrt(np.dot(np.ravel(l), np.ravel(l)))
        x0 = np.dot(x0, s)

    if not(verbose) or verbose:
        print('Initialization finished.\n')

    return x0
